{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3941fb40",
   "metadata": {},
   "source": [
    "Normalización del dataset combinando los aspectos mencionados en las especificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2ccedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir el directorio 'src' al sys.path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9009aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta absoluta de la carpeta 'src' para que sea accesible desde el notebook\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a8a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la función de normalización\n",
    "from preprocessing import normalize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8f8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas para cargar y manipular el dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd945c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../data/TA1C_dataset_detection_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c3dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar cada tipo de normalización\n",
    "df['tokenized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"tokenization\"))\n",
    "df['cleaned_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"text_cleaning\"))\n",
    "df['no_stopwords_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"remove_stopwords\"))\n",
    "df['lemmatized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"lemmatization\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5090b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos de normalización:\n",
      "                                         Teaser Text  \\\n",
      "0  #SegundaDivisión  | La fortaleza del ataque: R...   \n",
      "1  Jorge Lanata a los argentinos que se van a Uru...   \n",
      "2  Raffo: “Los montevideanos deben estar alerta p...   \n",
      "3  Ecos del universo: joven uruguayo desentraña (...   \n",
      "4  Propuesta quinquenal de ANEP: aumento de 3,8% ...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...   \n",
      "1  [Jorge, Lanata, a, los, argentinos, que, se, v...   \n",
      "2  [Raffo, :, “, Los, montevideanos, deben, estar...   \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...   \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  segundadivisión   la fortaleza del ataque ramp...   \n",
      "1  jorge lanata a los argentinos que se van a uru...   \n",
      "2  raffo “los montevideanos deben estar alerta po...   \n",
      "3  ecos del universo joven uruguayo desentraña y ...   \n",
      "4  propuesta quinquenal de anep aumento de  del p...   \n",
      "\n",
      "                                   no_stopwords_text  \\\n",
      "0  [#, SegundaDivisión,  , |, fortaleza, ataque, ...   \n",
      "1  [Jorge, Lanata, argentinos, Uruguay, :, \", Irs...   \n",
      "2  [Raffo, :, “, montevideanos, alerta, errores, ...   \n",
      "3  [Ecos, universo, :, joven, uruguayo, desentrañ...   \n",
      "4  [Propuesta, quinquenal, ANEP, :, aumento, 3,8%...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [#, segundadivisión,  , |, el, fortaleza, del,...  \n",
      "1  [Jorge, Lanata, a, el, argentino, que, él, ir,...  \n",
      "2  [Raffo, :, “, el, montevideano, deber, estar, ...  \n",
      "3  [eco, del, universo, :, joven, uruguayo, desen...  \n",
      "4  [propuesta, quinquenal, de, ANEP, :, aumento, ...  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos de cada tipo de normalización\n",
    "print(\"\\nEjemplos de normalización:\")\n",
    "print(df[['Teaser Text', 'tokenized_text', 'cleaned_text', 'no_stopwords_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b8b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f8828",
   "metadata": {},
   "source": [
    "Dividimos el dataset dentro del Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97d8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88902a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos de entrenamiento y desarrollo guardados en '../data/'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y desarrollo\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.25,  # 25% para desarrollo\n",
    "    random_state=0,  # Para reproducibilidad\n",
    "    stratify=df['Tag Value'],  # Estratificar según la columna de clase\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV\n",
    "train_df.to_csv(\"../data/TA1C_dataset_detection_train_split.csv\", index=False)\n",
    "dev_df.to_csv(\"../data/TA1C_dataset_detection_dev_split.csv\", index=False)\n",
    "\n",
    "print(\"Conjuntos de entrenamiento y desarrollo guardados en '../data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7d4c9",
   "metadata": {},
   "source": [
    "Representación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94fdc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'Tweet Date', 'Media Name', 'Media Origin', 'Teaser Text',\n",
      "       'Tag Value', 'tokenized_text', 'cleaned_text', 'no_stopwords_text',\n",
      "       'lemmatized_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar los conjuntos de entrenamiento y desarrollo\n",
    "train_df = pd.read_csv(\"../data/TA1C_dataset_detection_train_split.csv\")\n",
    "dev_df = pd.read_csv(\"../data/TA1C_dataset_detection_dev_split.csv\")\n",
    "\n",
    "# Verificar las columnas disponibles\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae418a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración: ngram_range=(1, 1), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 9183)\n",
      "Dimensiones de X_dev: (700, 9183)\n",
      "Configuración: ngram_range=(1, 2), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n",
      "Configuración: ngram_range=(1, 3), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 79900)\n",
      "Dimensiones de X_dev: (700, 79900)\n",
      "Configuración: ngram_range=(1, 2), representación=binary\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n",
      "Configuración: ngram_range=(1, 2), representación=frequency\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Probar diferentes configuraciones de n-gramas y representaciones\n",
    "configurations = [\n",
    "    {\"ngram_range\": (1, 1), \"representation\": \"tfidf\"},  # Unigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"tfidf\"},  # Unigramas + Bigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 3), \"representation\": \"tfidf\"},  # Unigramas + Bigramas + Trigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"binary\"},  # Unigramas + Bigramas con representación binaria\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"frequency\"},  # Unigramas + Bigramas con frecuencia\n",
    "]\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"Configuración: ngram_range={config['ngram_range']}, representación={config['representation']}\")\n",
    "    \n",
    "    # Configurar el vectorizador\n",
    "    if config[\"representation\"] == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"])\n",
    "    elif config[\"representation\"] == \"binary\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], binary=True)\n",
    "    elif config[\"representation\"] == \"frequency\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], use_idf=False)\n",
    "    \n",
    "    # Crear representaciones para el conjunto de entrenamiento y desarrollo\n",
    "    X_train = vectorizer.fit_transform(train_df['lemmatized_text'])\n",
    "    X_dev = vectorizer.transform(dev_df['lemmatized_text'])\n",
    "    \n",
    "    # Verificar las dimensiones\n",
    "    print(f\"Dimensiones de X_train: {X_train.shape}\")\n",
    "    print(f\"Dimensiones de X_dev: {X_dev.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa5a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones reducidas de X_train: (2100, 100)\n",
      "Dimensiones reducidas de X_dev: (700, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reducir dimensionalidad con TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=0)  # Reducir a 100 dimensiones\n",
    "X_train_reduced = svd.fit_transform(X_train)\n",
    "X_dev_reduced = svd.transform(X_dev)\n",
    "\n",
    "# Verificar las dimensiones después de la reducción\n",
    "print(f\"Dimensiones reducidas de X_train: {X_train_reduced.shape}\")\n",
    "print(f\"Dimensiones reducidas de X_dev: {X_dev_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f25a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas de texto a valores numéricos\n",
    "label_mapping = {'Clickbait': 1, 'No': 0}\n",
    "train_df['Tag Value'] = train_df['Tag Value'].map(label_mapping)\n",
    "dev_df['Tag Value'] = dev_df['Tag Value'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378fa2e",
   "metadata": {},
   "source": [
    "Logistic Regression y Validación Cruada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d13f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro scores por fold: [0.57350414 0.57255747 0.61648997 0.60008246 0.55428571]\n",
      "F1-macro promedio: 0.5834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Configurar validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando f1_macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "scores = cross_val_score(model, X_train_reduced, train_df['Tag Value'], cv=cv, scoring=f1_macro_scorer)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"F1-macro scores por fold: {scores}\")\n",
    "print(f\"F1-macro promedio: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717a2d",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo utilizando algoritmos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679ac81",
   "metadata": {},
   "source": [
    "Naive Bayes Multinominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f64e762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Multinomial (sin TruncatedSVD):\n",
      "F1-macro scores por fold: [0.41666667 0.41666667 0.42551064 0.41747573 0.42638889]\n",
      "F1-macro promedio: 0.4205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Naïve Bayes Multinomial\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones originales (X_train)\n",
    "nb_scores = cross_val_score(nb_model, X_train, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Naïve Bayes Multinomial (sin TruncatedSVD):\")\n",
    "print(f\"F1-macro scores por fold: {nb_scores}\")\n",
    "print(f\"F1-macro promedio: {nb_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ccb03",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d19ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Mejor configuración encontrada:\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Mejor F1-macro: 0.7048\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.4241 (+/-0.0037)\n",
      "{'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.6204 (+/-0.0073)\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.6788 (+/-0.0196)\n",
      "{'C': 1, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.6204 (+/-0.0073)\n",
      "{'C': 1, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.7048 (+/-0.0241)\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.7041 (+/-0.0149)\n",
      "{'C': 10, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.7048 (+/-0.0241)\n",
      "{'C': 10, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4481 (+/-0.0137)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "svc = SVC(random_state=0)\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    svc,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b14ebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines (SVC):\n",
      "F1-macro scores por fold: [0.72859226 0.67157895 0.71005814 0.68562874 0.71975417]\n",
      "F1-macro promedio: 0.7031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de SVC\n",
    "svc_model = SVC(C=10,gamma='scale',kernel='linear', random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "svc_scores = cross_val_score(svc_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Support Vector Machines (SVC):\")\n",
    "print(f\"F1-macro scores por fold: {svc_scores}\")\n",
    "print(f\"F1-macro promedio: {svc_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00becf75",
   "metadata": {},
   "source": [
    "Multi-layer Perceptron (MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77984377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Mejor configuración encontrada:\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'}\n",
      "Mejor F1-macro: 0.7230\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.7101 (+/-0.0143)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.6970 (+/-0.0244)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.7215 (+/-0.0200)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.6858 (+/-0.0213)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.6975 (+/-0.0269)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.6808 (+/-0.0161)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.7147 (+/-0.0264)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.6776 (+/-0.0158)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.7092 (+/-0.0193)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.6827 (+/-0.0127)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.7212 (+/-0.0200)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.6791 (+/-0.0104)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.7006 (+/-0.0251)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.6748 (+/-0.0154)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.7188 (+/-0.0246)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.6821 (+/-0.0212)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.7141 (+/-0.0233)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.6889 (+/-0.0186)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.7230 (+/-0.0244)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.6856 (+/-0.0145)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.7085 (+/-0.0303)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.6749 (+/-0.0191)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.7213 (+/-0.0199)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.6961 (+/-0.0221)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "mlp = MLPClassifier(random_state=0, max_iter=1000)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9812885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron (MLPClassifier):\n",
      "F1-macro scores por fold: [0.75745437 0.69378876 0.73137096 0.73060439 0.68818717]\n",
      "F1-macro promedio: 0.7203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=0.001, solver='adam', random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "mlp_scores = cross_val_score(mlp_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Multi-layer Perceptron (MLPClassifier):\")\n",
    "print(f\"F1-macro scores por fold: {mlp_scores}\")\n",
    "print(f\"F1-macro promedio: {mlp_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242fcb6a",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "770eebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Mejor configuración encontrada:\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Mejor F1-macro: 0.6510\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6260 (+/-0.0191)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6370 (+/-0.0192)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6388 (+/-0.0159)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6307 (+/-0.0199)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6375 (+/-0.0178)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6391 (+/-0.0150)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6505 (+/-0.0208)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6426 (+/-0.0165)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6415 (+/-0.0213)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6280 (+/-0.0132)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6275 (+/-0.0177)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6323 (+/-0.0118)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6305 (+/-0.0220)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6333 (+/-0.0113)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6270 (+/-0.0191)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6378 (+/-0.0246)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6369 (+/-0.0167)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6360 (+/-0.0184)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6414 (+/-0.0180)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6249 (+/-0.0154)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6414 (+/-0.0180)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6249 (+/-0.0154)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6375 (+/-0.0181)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6306 (+/-0.0144)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6236 (+/-0.0152)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6117 (+/-0.0182)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6181 (+/-0.0189)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6196 (+/-0.0145)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6218 (+/-0.0202)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6163 (+/-0.0173)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6204 (+/-0.0161)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6211 (+/-0.0139)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6161 (+/-0.0189)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6187 (+/-0.0128)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6223 (+/-0.0160)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6150 (+/-0.0161)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6211 (+/-0.0201)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6094 (+/-0.0158)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6154 (+/-0.0175)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6167 (+/-0.0172)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6151 (+/-0.0244)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6173 (+/-0.0094)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6147 (+/-0.0148)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6180 (+/-0.0145)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6067 (+/-0.0234)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6079 (+/-0.0095)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6180 (+/-0.0145)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6067 (+/-0.0234)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6079 (+/-0.0095)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6205 (+/-0.0162)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6155 (+/-0.0156)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6155 (+/-0.0153)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6276 (+/-0.0165)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6425 (+/-0.0173)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6359 (+/-0.0138)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6412 (+/-0.0179)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6307 (+/-0.0211)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6375 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6425 (+/-0.0195)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6401 (+/-0.0195)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6398 (+/-0.0161)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6294 (+/-0.0081)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6287 (+/-0.0181)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6337 (+/-0.0147)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6303 (+/-0.0188)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6389 (+/-0.0140)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6290 (+/-0.0178)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6416 (+/-0.0174)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6411 (+/-0.0147)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6319 (+/-0.0139)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6344 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6233 (+/-0.0131)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6344 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6233 (+/-0.0131)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6408 (+/-0.0163)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6317 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6284 (+/-0.0122)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6282 (+/-0.0215)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6366 (+/-0.0197)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6395 (+/-0.0178)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6308 (+/-0.0198)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6395 (+/-0.0177)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6402 (+/-0.0155)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6510 (+/-0.0214)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6422 (+/-0.0169)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6431 (+/-0.0202)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6280 (+/-0.0132)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6275 (+/-0.0177)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6323 (+/-0.0118)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6305 (+/-0.0220)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6333 (+/-0.0113)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6270 (+/-0.0191)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6378 (+/-0.0246)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6369 (+/-0.0167)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6360 (+/-0.0184)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.6414 (+/-0.0180)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.6249 (+/-0.0154)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.6414 (+/-0.0180)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.6319 (+/-0.0184)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.6249 (+/-0.0154)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.6375 (+/-0.0181)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.6306 (+/-0.0144)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.6236 (+/-0.0152)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce05bca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "F1-macro scores por fold: [0.66965786 0.60441819 0.63211679 0.64269706 0.64793972]\n",
      "F1-macro promedio: 0.6394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(max_depth=30, min_samples_leaf=1, min_samples_split=10,n_estimators=50, random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "rf_scores = cross_val_score(rf_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Random Forest:\")\n",
    "print(f\"F1-macro scores por fold: {rf_scores}\")\n",
    "print(f\"F1-macro promedio: {rf_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163749c1",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c60263a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Mejor configuración encontrada:\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Mejor F1-macro: 0.7020\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.5368 (+/-0.0223)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.5362 (+/-0.0219)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5558 (+/-0.0228)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5548 (+/-0.0243)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.5877 (+/-0.0317)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.5859 (+/-0.0305)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.5326 (+/-0.0169)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.5463 (+/-0.0236)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5632 (+/-0.0259)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5716 (+/-0.0229)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6216 (+/-0.0113)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6255 (+/-0.0142)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.5324 (+/-0.0164)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.5481 (+/-0.0301)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5961 (+/-0.0119)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5973 (+/-0.0228)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6492 (+/-0.0138)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6383 (+/-0.0209)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6026 (+/-0.0200)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6043 (+/-0.0282)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6408 (+/-0.0078)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6372 (+/-0.0147)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6696 (+/-0.0125)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6636 (+/-0.0105)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6402 (+/-0.0151)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6395 (+/-0.0108)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6705 (+/-0.0114)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6609 (+/-0.0109)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6862 (+/-0.0149)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6798 (+/-0.0079)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6614 (+/-0.0096)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6651 (+/-0.0134)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6761 (+/-0.0178)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6812 (+/-0.0061)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6845 (+/-0.0140)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6929 (+/-0.0180)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6517 (+/-0.0084)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6404 (+/-0.0117)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6809 (+/-0.0193)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6597 (+/-0.0128)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6908 (+/-0.0231)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6792 (+/-0.0188)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6700 (+/-0.0213)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6644 (+/-0.0104)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6916 (+/-0.0187)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6831 (+/-0.0136)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.7020 (+/-0.0248)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6958 (+/-0.0180)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6818 (+/-0.0205)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6860 (+/-0.0123)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6857 (+/-0.0155)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6912 (+/-0.0170)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6914 (+/-0.0162)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6992 (+/-0.0184)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    gb,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a4cabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting:\n",
      "F1-macro scores por fold: [0.68047025 0.63653528 0.67208663 0.67474747 0.66369586]\n",
      "F1-macro promedio: 0.6655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(subsample=1.0,n_estimators=200, learning_rate=0.1, max_depth=5, random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "gb_scores = cross_val_score(gb_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Gradient Boosting:\")\n",
    "print(f\"F1-macro scores por fold: {gb_scores}\")\n",
    "print(f\"F1-macro promedio: {gb_scores.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
