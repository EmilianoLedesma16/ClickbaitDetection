{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2ccedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir el directorio 'src' al sys.path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9009aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta absoluta de la carpeta 'src' para que sea accesible desde el notebook\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a8a6c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importar la función de normalización\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\src\\preprocessing.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Cargar el modelo de lenguaje en inglés\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mes_core_web_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize_text\u001b[39m(text, mode):\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Normaliza el texto según el modo especificado.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    Modes:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m        - \"lemmatization\": Aplica lematización.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     28\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     29\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     35\u001b[39m ) -> Language:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\spacy\\util.py:472\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'es_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Importar la función de normalización\n",
    "from preprocessing import normalize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas para cargar y manipular el dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd945c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../data/TA1C_dataset_detection_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar cada tipo de normalización\n",
    "df['tokenized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"tokenization\"))\n",
    "df['cleaned_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"text_cleaning\"))\n",
    "df['no_stopwords_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"remove_stopwords\"))\n",
    "df['lemmatized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"lemmatization\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos de normalización:\n",
      "                                         Teaser Text  \\\n",
      "0  #SegundaDivisión  | La fortaleza del ataque: R...   \n",
      "1  Jorge Lanata a los argentinos que se van a Uru...   \n",
      "2  Raffo: “Los montevideanos deben estar alerta p...   \n",
      "3  Ecos del universo: joven uruguayo desentraña (...   \n",
      "4  Propuesta quinquenal de ANEP: aumento de 3,8% ...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...   \n",
      "1  [Jorge, Lanata, a, los, argentinos, que, se, v...   \n",
      "2  [Raffo, :, “, Los, montevideanos, deben, estar...   \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...   \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  segundadivisión   la fortaleza del ataque ramp...   \n",
      "1  jorge lanata a los argentinos que se van a uru...   \n",
      "2  raffo “los montevideanos deben estar alerta po...   \n",
      "3  ecos del universo joven uruguayo desentraña y ...   \n",
      "4  propuesta quinquenal de anep aumento de  del p...   \n",
      "\n",
      "                                   no_stopwords_text  \\\n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...   \n",
      "1  [Jorge, Lanata, los, argentinos, que, se, van,...   \n",
      "2  [Raffo, :, “, Los, montevideanos, deben, estar...   \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...   \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...  \n",
      "1  [Jorge, Lanata, a, los, argentinos, que, se, v...  \n",
      "2  [raffo, :, \", los, montevideano, deben, estar,...  \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...  \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos de cada tipo de normalización\n",
    "print(\"\\nEjemplos de normalización:\")\n",
    "print(df[['Teaser Text', 'tokenized_text', 'cleaned_text', 'no_stopwords_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b8b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
