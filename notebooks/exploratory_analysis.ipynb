{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3941fb40",
   "metadata": {},
   "source": [
    "Normalización del dataset combinando los aspectos mencionados en las especificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2ccedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir el directorio 'src' al sys.path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9009aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta absoluta de la carpeta 'src' para que sea accesible desde el notebook\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a8a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la función de normalización\n",
    "from preprocessing import normalize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8f8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas para cargar y manipular el dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd945c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../data/TA1C_dataset_detection_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c3dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar cada tipo de normalización\n",
    "df['tokenized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"tokenization\"))\n",
    "df['cleaned_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"text_cleaning\"))\n",
    "df['no_stopwords_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"remove_stopwords\"))\n",
    "df['lemmatized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"lemmatization\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5090b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos de normalización:\n",
      "                                         Teaser Text  \\\n",
      "0  #SegundaDivisión  | La fortaleza del ataque: R...   \n",
      "1  Jorge Lanata a los argentinos que se van a Uru...   \n",
      "2  Raffo: “Los montevideanos deben estar alerta p...   \n",
      "3  Ecos del universo: joven uruguayo desentraña (...   \n",
      "4  Propuesta quinquenal de ANEP: aumento de 3,8% ...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...   \n",
      "1  [Jorge, Lanata, a, los, argentinos, que, se, v...   \n",
      "2  [Raffo, :, “, Los, montevideanos, deben, estar...   \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...   \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  segundadivisión   la fortaleza del ataque ramp...   \n",
      "1  jorge lanata a los argentinos que se van a uru...   \n",
      "2  raffo “los montevideanos deben estar alerta po...   \n",
      "3  ecos del universo joven uruguayo desentraña y ...   \n",
      "4  propuesta quinquenal de anep aumento de  del p...   \n",
      "\n",
      "                                   no_stopwords_text  \\\n",
      "0  [#, SegundaDivisión,  , |, fortaleza, ataque, ...   \n",
      "1  [Jorge, Lanata, argentinos, Uruguay, :, \", Irs...   \n",
      "2  [Raffo, :, “, montevideanos, alerta, errores, ...   \n",
      "3  [Ecos, universo, :, joven, uruguayo, desentrañ...   \n",
      "4  [Propuesta, quinquenal, ANEP, :, aumento, 3,8%...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [#, segundadivisión,  , |, el, fortaleza, del,...  \n",
      "1  [Jorge, Lanata, a, el, argentino, que, él, ir,...  \n",
      "2  [Raffo, :, “, el, montevideano, deber, estar, ...  \n",
      "3  [eco, del, universo, :, joven, uruguayo, desen...  \n",
      "4  [propuesta, quinquenal, de, ANEP, :, aumento, ...  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos de cada tipo de normalización\n",
    "print(\"\\nEjemplos de normalización:\")\n",
    "print(df[['Teaser Text', 'tokenized_text', 'cleaned_text', 'no_stopwords_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b8b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f8828",
   "metadata": {},
   "source": [
    "Dividimos el dataset dentro del Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97d8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88902a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos de entrenamiento y desarrollo guardados en '../data/'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y desarrollo\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.25,  # 25% para desarrollo\n",
    "    random_state=0,  # Para reproducibilidad\n",
    "    stratify=df['Tag Value']  # Estratificar según la columna de clase\n",
    ")\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV\n",
    "train_df.to_csv(\"../data/TA1C_dataset_detection_train_split.csv\", index=False)\n",
    "dev_df.to_csv(\"../data/TA1C_dataset_detection_dev_split.csv\", index=False)\n",
    "\n",
    "print(\"Conjuntos de entrenamiento y desarrollo guardados en '../data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7d4c9",
   "metadata": {},
   "source": [
    "Representación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94fdc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'Tweet Date', 'Media Name', 'Media Origin', 'Teaser Text',\n",
      "       'Tag Value', 'tokenized_text', 'cleaned_text', 'no_stopwords_text',\n",
      "       'lemmatized_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar los conjuntos de entrenamiento y desarrollo\n",
    "train_df = pd.read_csv(\"../data/TA1C_dataset_detection_train_split.csv\")\n",
    "dev_df = pd.read_csv(\"../data/TA1C_dataset_detection_dev_split.csv\")\n",
    "\n",
    "# Verificar las columnas disponibles\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dae418a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración: ngram_range=(1, 1), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 9183)\n",
      "Dimensiones de X_dev: (700, 9183)\n",
      "Configuración: ngram_range=(1, 2), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n",
      "Configuración: ngram_range=(1, 3), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 79900)\n",
      "Dimensiones de X_dev: (700, 79900)\n",
      "Configuración: ngram_range=(1, 2), representación=binary\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n",
      "Configuración: ngram_range=(1, 2), representación=frequency\n",
      "Dimensiones de X_train: (2100, 39506)\n",
      "Dimensiones de X_dev: (700, 39506)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Probar diferentes configuraciones de n-gramas y representaciones\n",
    "configurations = [\n",
    "    {\"ngram_range\": (1, 1), \"representation\": \"tfidf\"},  # Unigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"tfidf\"},  # Unigramas + Bigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 3), \"representation\": \"tfidf\"},  # Unigramas + Bigramas + Trigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"binary\"},  # Unigramas + Bigramas con representación binaria\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"frequency\"},  # Unigramas + Bigramas con frecuencia\n",
    "]\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"Configuración: ngram_range={config['ngram_range']}, representación={config['representation']}\")\n",
    "    \n",
    "    # Configurar el vectorizador\n",
    "    if config[\"representation\"] == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"])\n",
    "    elif config[\"representation\"] == \"binary\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], binary=True)\n",
    "    elif config[\"representation\"] == \"frequency\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], use_idf=False)\n",
    "    \n",
    "    # Crear representaciones para el conjunto de entrenamiento y desarrollo\n",
    "    X_train = vectorizer.fit_transform(train_df['lemmatized_text'])\n",
    "    X_dev = vectorizer.transform(dev_df['lemmatized_text'])\n",
    "    \n",
    "    # Verificar las dimensiones\n",
    "    print(f\"Dimensiones de X_train: {X_train.shape}\")\n",
    "    print(f\"Dimensiones de X_dev: {X_dev.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa5a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones reducidas de X_train: (2100, 100)\n",
      "Dimensiones reducidas de X_dev: (700, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reducir dimensionalidad con TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=0)  # Reducir a 100 dimensiones\n",
    "X_train_reduced = svd.fit_transform(X_train)\n",
    "X_dev_reduced = svd.transform(X_dev)\n",
    "\n",
    "# Verificar las dimensiones después de la reducción\n",
    "print(f\"Dimensiones reducidas de X_train: {X_train_reduced.shape}\")\n",
    "print(f\"Dimensiones reducidas de X_dev: {X_dev_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f25a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas de texto a valores numéricos\n",
    "label_mapping = {'Clickbait': 1, 'No': 0}\n",
    "train_df['Tag Value'] = train_df['Tag Value'].map(label_mapping)\n",
    "dev_df['Tag Value'] = dev_df['Tag Value'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378fa2e",
   "metadata": {},
   "source": [
    "Validación Cruada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d13f932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro scores por fold: [0.57350414 0.57255747 0.61648997 0.60008246 0.55428571]\n",
      "F1-macro promedio: 0.5834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Configurar validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando f1_macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "scores = cross_val_score(model, X_train_reduced, train_df['Tag Value'], cv=cv, scoring=f1_macro_scorer)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"F1-macro scores por fold: {scores}\")\n",
    "print(f\"F1-macro promedio: {scores.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
