{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3941fb40",
   "metadata": {},
   "source": [
    "Normalización del dataset combinando los aspectos mencionados en las especificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2ccedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir el directorio 'src' al sys.path\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9009aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta absoluta de la carpeta 'src' para que sea accesible desde el notebook\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a8a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la función de normalización\n",
    "from preprocessing import normalize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8f8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas para cargar y manipular el dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd945c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../data/TA1C_dataset_detection_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c3dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar cada tipo de normalización\n",
    "df['tokenized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"tokenization\"))\n",
    "df['cleaned_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"text_cleaning\"))\n",
    "df['no_stopwords_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"remove_stopwords\"))\n",
    "df['lemmatized_text'] = df['Teaser Text'].apply(lambda x: normalize_text(x, mode=\"lemmatization\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5090b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos de normalización:\n",
      "                                         Teaser Text  \\\n",
      "0  #SegundaDivisión  | La fortaleza del ataque: R...   \n",
      "1  Jorge Lanata a los argentinos que se van a Uru...   \n",
      "2  Raffo: “Los montevideanos deben estar alerta p...   \n",
      "3  Ecos del universo: joven uruguayo desentraña (...   \n",
      "4  Propuesta quinquenal de ANEP: aumento de 3,8% ...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [#, SegundaDivisión,  , |, La, fortaleza, del,...   \n",
      "1  [Jorge, Lanata, a, los, argentinos, que, se, v...   \n",
      "2  [Raffo, :, “, Los, montevideanos, deben, estar...   \n",
      "3  [Ecos, del, universo, :, joven, uruguayo, dese...   \n",
      "4  [Propuesta, quinquenal, de, ANEP, :, aumento, ...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  segundadivisión   la fortaleza del ataque ramp...   \n",
      "1  jorge lanata a los argentinos que se van a uru...   \n",
      "2  raffo “los montevideanos deben estar alerta po...   \n",
      "3  ecos del universo joven uruguayo desentraña y ...   \n",
      "4  propuesta quinquenal de anep aumento de  del p...   \n",
      "\n",
      "                                   no_stopwords_text  \\\n",
      "0  [#, SegundaDivisión,  , |, fortaleza, ataque, ...   \n",
      "1  [Jorge, Lanata, argentinos, Uruguay, :, \", Irs...   \n",
      "2  [Raffo, :, “, montevideanos, alerta, errores, ...   \n",
      "3  [Ecos, universo, :, joven, uruguayo, desentrañ...   \n",
      "4  [Propuesta, quinquenal, ANEP, :, aumento, 3,8%...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [#, segundadivisión,  , |, el, fortaleza, del,...  \n",
      "1  [Jorge, Lanata, a, el, argentino, que, él, ir,...  \n",
      "2  [Raffo, :, “, el, montevideano, deber, estar, ...  \n",
      "3  [eco, del, universo, :, joven, uruguayo, desen...  \n",
      "4  [propuesta, quinquenal, de, ANEP, :, aumento, ...  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos de cada tipo de normalización\n",
    "print(\"\\nEjemplos de normalización:\")\n",
    "print(df[['Teaser Text', 'tokenized_text', 'cleaned_text', 'no_stopwords_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b8b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f8828",
   "metadata": {},
   "source": [
    "Dividimos el dataset dentro del Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97d8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con normalizaciones guardado en '../data/TA1C_dataset_detection_train_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset con todas las columnas de normalización\n",
    "output_path = \"../data/TA1C_dataset_detection_train_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Dataset con normalizaciones guardado en '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88902a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos de entrenamiento y desarrollo guardados en '../data/'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir el dataset en conjuntos de entrenamiento y desarrollo\n",
    "train_df, dev_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.25,  # 25% para desarrollo\n",
    "    random_state=0,  # Semilla fija para reproducibilidad\n",
    "    stratify=df['Tag Value']  # Estratificar según la columna de clase\n",
    ")\n",
    "\n",
    "# Guardar los conjuntos en archivos CSV\n",
    "train_df.to_csv(\"../data/TA1C_dataset_detection_train_split.csv\", index=False)\n",
    "dev_df.to_csv(\"../data/TA1C_dataset_detection_dev_split.csv\", index=False)\n",
    "\n",
    "print(\"Conjuntos de entrenamiento y desarrollo guardados en '../data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7d4c9",
   "metadata": {},
   "source": [
    "Representación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886b95e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'Tweet Date', 'Media Name', 'Media Origin', 'Teaser Text',\n",
      "       'Tag Value', 'tokenized_text', 'cleaned_text', 'no_stopwords_text',\n",
      "       'lemmatized_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar los conjuntos de entrenamiento y desarrollo\n",
    "train_df = pd.read_csv(\"../data/TA1C_dataset_detection_train_split.csv\")\n",
    "dev_df = pd.read_csv(\"../data/TA1C_dataset_detection_dev_split.csv\")\n",
    "\n",
    "# Verificar las columnas disponibles\n",
    "print(train_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
