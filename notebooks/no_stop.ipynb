{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d03c804",
   "metadata": {},
   "source": [
    "# Entranamiento de modelos con no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6878b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tweet ID', 'Teaser Text', 'Tag Value', 'tokenized_text',\n",
      "       'cleaned_text', 'no_stopwords_text', 'lemmatized_text',\n",
      "       'tokenized_cleaned_text', 'tokenized_cleaned_text_no_stopwords',\n",
      "       'tokenized_cleaned_text_no_stopwords_lemmatized'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los conjuntos de entrenamiento y desarrollo\n",
    "train_df = pd.read_csv(\"../data/TA1C_dataset_detection_train_split.csv\")\n",
    "dev_df = pd.read_csv(\"../data/TA1C_dataset_detection_dev_split.csv\")\n",
    "\n",
    "# Verificar las columnas disponibles\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa878715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración: ngram_range=(1, 1), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 11043)\n",
      "Dimensiones de X_dev: (700, 11043)\n",
      "Configuración: ngram_range=(1, 2), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 33795)\n",
      "Dimensiones de X_dev: (700, 33795)\n",
      "Configuración: ngram_range=(1, 3), representación=tfidf\n",
      "Dimensiones de X_train: (2100, 56344)\n",
      "Dimensiones de X_dev: (700, 56344)\n",
      "Configuración: ngram_range=(1, 2), representación=binary\n",
      "Dimensiones de X_train: (2100, 33795)\n",
      "Dimensiones de X_dev: (700, 33795)\n",
      "Configuración: ngram_range=(1, 2), representación=frequency\n",
      "Dimensiones de X_train: (2100, 33795)\n",
      "Dimensiones de X_dev: (700, 33795)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Probar diferentes configuraciones de n-gramas y representaciones\n",
    "configurations = [\n",
    "    {\"ngram_range\": (1, 1), \"representation\": \"tfidf\"},  # Unigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"tfidf\"},  # Unigramas + Bigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 3), \"representation\": \"tfidf\"},  # Unigramas + Bigramas + Trigramas con TF-IDF\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"binary\"},  # Unigramas + Bigramas con representación binaria\n",
    "    {\"ngram_range\": (1, 2), \"representation\": \"frequency\"},  # Unigramas + Bigramas con frecuencia\n",
    "]\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"Configuración: ngram_range={config['ngram_range']}, representación={config['representation']}\")\n",
    "    \n",
    "    # Configurar el vectorizador\n",
    "    if config[\"representation\"] == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"])\n",
    "    elif config[\"representation\"] == \"binary\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], binary=True)\n",
    "    elif config[\"representation\"] == \"frequency\":\n",
    "        vectorizer = TfidfVectorizer(ngram_range=config[\"ngram_range\"], use_idf=False)\n",
    "    \n",
    "    # Crear representaciones para el conjunto de entrenamiento y desarrollo\n",
    "    X_train = vectorizer.fit_transform(train_df['no_stopwords_text'])\n",
    "    X_dev = vectorizer.transform(dev_df['no_stopwords_text'])\n",
    "    \n",
    "    # Verificar las dimensiones\n",
    "    print(f\"Dimensiones de X_train: {X_train.shape}\")\n",
    "    print(f\"Dimensiones de X_dev: {X_dev.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f703cf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones reducidas de X_train: (2100, 100)\n",
      "Dimensiones reducidas de X_dev: (700, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reducir dimensionalidad con TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, random_state=0)  # Reducir a 100 dimensiones\n",
    "X_train_reduced = svd.fit_transform(X_train)\n",
    "X_dev_reduced = svd.transform(X_dev)\n",
    "\n",
    "# Verificar las dimensiones después de la reducción\n",
    "print(f\"Dimensiones reducidas de X_train: {X_train_reduced.shape}\")\n",
    "print(f\"Dimensiones reducidas de X_dev: {X_dev_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdaa0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas de texto a valores numéricos\n",
    "label_mapping = {'Clickbait': 1, 'No': 0}\n",
    "train_df['Tag Value'] = train_df['Tag Value'].map(label_mapping)\n",
    "dev_df['Tag Value'] = dev_df['Tag Value'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b25d38",
   "metadata": {},
   "source": [
    "## Logistic Regression y Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0383e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro scores por fold: [0.41666667 0.41585535 0.43422074 0.41585535 0.41747573]\n",
      "F1-macro promedio: 0.4200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Configurar validación cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Definir el modelo\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando f1_macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "scores = cross_val_score(model, X_train_reduced, train_df['Tag Value'], cv=cv, scoring=f1_macro_scorer)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"F1-macro scores por fold: {scores}\")\n",
    "print(f\"F1-macro promedio: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badadf0",
   "metadata": {},
   "source": [
    "## Naive Bayes Multinominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da1759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Multinomial (sin TruncatedSVD):\n",
      "F1-macro scores por fold: [0.41666667 0.41666667 0.43422074 0.41747573 0.42638889]\n",
      "F1-macro promedio: 0.4223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Naïve Bayes Multinomial\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones originales (X_train)\n",
    "nb_scores = cross_val_score(nb_model, X_train, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Naïve Bayes Multinomial (sin TruncatedSVD):\")\n",
    "print(f\"F1-macro scores por fold: {nb_scores}\")\n",
    "print(f\"F1-macro promedio: {nb_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189668",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589b0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Mejor configuración encontrada:\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Mejor F1-macro: 0.5842\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.4773 (+/-0.0181)\n",
      "{'C': 1, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 1, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'linear'} -> F1-macro: 0.4502 (+/-0.0134)\n",
      "{'C': 10, 'gamma': 'scale', 'kernel': 'rbf'} -> F1-macro: 0.5842 (+/-0.0384)\n",
      "{'C': 10, 'gamma': 'auto', 'kernel': 'linear'} -> F1-macro: 0.4502 (+/-0.0134)\n",
      "{'C': 10, 'gamma': 'auto', 'kernel': 'rbf'} -> F1-macro: 0.4170 (+/-0.0004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "svc = SVC(random_state=0)\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    svc,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98aa1724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines (SVC):\n",
      "F1-macro scores por fold: [0.59490741 0.56386293 0.63195996 0.57103189 0.55265834]\n",
      "F1-macro promedio: 0.5829\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de SVC\n",
    "svc_model = SVC(C=10,gamma='scale',kernel='rbf', random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "svc_scores = cross_val_score(svc_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Support Vector Machines (SVC):\")\n",
    "print(f\"F1-macro scores por fold: {svc_scores}\")\n",
    "print(f\"F1-macro promedio: {svc_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15aa1190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7274    0.8540    0.7856       500\n",
      "           1     0.3540    0.2000    0.2556       200\n",
      "\n",
      "    accuracy                         0.6671       700\n",
      "   macro avg     0.5407    0.5270    0.5206       700\n",
      "weighted avg     0.6207    0.6671    0.6342       700\n",
      "\n",
      "Matriz de confusión:\n",
      "[[427  73]\n",
      " [160  40]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de entrenamiento\n",
    "svc_model.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Predecir sobre el conjunto de validación/desarrollo\n",
    "y_pred = svc_model.predict(X_dev_reduced)\n",
    "y_true = dev_df['Tag Value']\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f76969",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron (MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82a627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Mejor configuración encontrada:\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (150,), 'solver': 'adam'}\n",
      "Mejor F1-macro: 0.6078\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.5869 (+/-0.0296)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.5846 (+/-0.0227)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.5886 (+/-0.0245)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.5730 (+/-0.0141)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.5876 (+/-0.0096)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.5838 (+/-0.0232)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.6036 (+/-0.0214)\n",
      "{'alpha': 0.0001, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.5710 (+/-0.0269)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.5884 (+/-0.0151)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.5728 (+/-0.0299)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.5972 (+/-0.0235)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.5807 (+/-0.0266)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.5813 (+/-0.0142)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.5737 (+/-0.0161)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.6031 (+/-0.0200)\n",
      "{'alpha': 0.001, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.5818 (+/-0.0203)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (50,), 'solver': 'adam'} -> F1-macro: 0.5913 (+/-0.0278)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (50,), 'solver': 'lbfgs'} -> F1-macro: 0.5646 (+/-0.0266)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'adam'} -> F1-macro: 0.6062 (+/-0.0268)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100,), 'solver': 'lbfgs'} -> F1-macro: 0.5818 (+/-0.0156)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100, 50), 'solver': 'adam'} -> F1-macro: 0.5801 (+/-0.0282)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (100, 50), 'solver': 'lbfgs'} -> F1-macro: 0.5760 (+/-0.0143)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (150,), 'solver': 'adam'} -> F1-macro: 0.6078 (+/-0.0267)\n",
      "{'alpha': 0.01, 'hidden_layer_sizes': (150,), 'solver': 'lbfgs'} -> F1-macro: 0.5817 (+/-0.0265)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "mlp = MLPClassifier(random_state=0, max_iter=2000)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    mlp,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d3609e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron (MLPClassifier):\n",
      "F1-macro scores por fold: [0.54080294 0.54248366 0.61451247 0.58089808 0.58616191]\n",
      "F1-macro promedio: 0.5730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(150,), max_iter=2000, alpha=0.001, solver='lbfgs', random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "mlp_scores = cross_val_score(mlp_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Multi-layer Perceptron (MLPClassifier):\")\n",
    "print(f\"F1-macro scores por fold: {mlp_scores}\")\n",
    "print(f\"F1-macro promedio: {mlp_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04657299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7679    0.8140    0.7903       500\n",
      "           1     0.4529    0.3850    0.4162       200\n",
      "\n",
      "    accuracy                         0.6914       700\n",
      "   macro avg     0.6104    0.5995    0.6033       700\n",
      "weighted avg     0.6779    0.6914    0.6834       700\n",
      "\n",
      "Matriz de confusión:\n",
      "[[407  93]\n",
      " [123  77]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emili\\OneDrive\\Documentos\\ESCOM\\OCTAVO_SEMESTRE\\NLP\\clickbait_detection\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de entrenamiento\n",
    "mlp_model.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Predecir sobre el conjunto de validación/desarrollo\n",
    "y_pred = mlp_model.predict(X_dev_reduced)\n",
    "y_true = dev_df['Tag Value']\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09802203",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68122211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Mejor configuración encontrada:\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Mejor F1-macro: 0.5224\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5092 (+/-0.0106)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.5011 (+/-0.0206)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.5031 (+/-0.0178)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.5166 (+/-0.0092)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.5051 (+/-0.0125)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.5025 (+/-0.0173)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5021 (+/-0.0198)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4949 (+/-0.0156)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.5003 (+/-0.0187)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5122 (+/-0.0156)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.5011 (+/-0.0146)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4949 (+/-0.0062)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.5001 (+/-0.0106)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.5031 (+/-0.0140)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4903 (+/-0.0067)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5012 (+/-0.0163)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4902 (+/-0.0064)\n",
      "{'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4784 (+/-0.0055)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4859 (+/-0.0132)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4814 (+/-0.0090)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4798 (+/-0.0108)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4859 (+/-0.0132)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4814 (+/-0.0090)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4798 (+/-0.0108)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4928 (+/-0.0093)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4703 (+/-0.0159)\n",
      "{'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4668 (+/-0.0077)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4697 (+/-0.0135)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4719 (+/-0.0125)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4638 (+/-0.0119)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4610 (+/-0.0041)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4602 (+/-0.0043)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4571 (+/-0.0085)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4616 (+/-0.0067)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4617 (+/-0.0145)\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4569 (+/-0.0147)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4679 (+/-0.0087)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4595 (+/-0.0181)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4558 (+/-0.0155)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4660 (+/-0.0182)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4607 (+/-0.0116)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4609 (+/-0.0182)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4658 (+/-0.0076)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4663 (+/-0.0134)\n",
      "{'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4579 (+/-0.0056)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4600 (+/-0.0062)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4533 (+/-0.0144)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4478 (+/-0.0076)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4600 (+/-0.0062)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4533 (+/-0.0144)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4478 (+/-0.0076)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4629 (+/-0.0113)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4519 (+/-0.0099)\n",
      "{'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4482 (+/-0.0052)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5224 (+/-0.0166)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.5037 (+/-0.0134)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.5101 (+/-0.0133)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.5163 (+/-0.0157)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4938 (+/-0.0070)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.5021 (+/-0.0156)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5040 (+/-0.0113)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4936 (+/-0.0198)\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4968 (+/-0.0267)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5046 (+/-0.0157)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4974 (+/-0.0118)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4943 (+/-0.0091)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4946 (+/-0.0157)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4976 (+/-0.0154)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4912 (+/-0.0093)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5019 (+/-0.0194)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4867 (+/-0.0056)\n",
      "{'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4770 (+/-0.0087)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4829 (+/-0.0037)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4793 (+/-0.0132)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4765 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4829 (+/-0.0037)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4793 (+/-0.0132)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4765 (+/-0.0160)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4881 (+/-0.0091)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4701 (+/-0.0152)\n",
      "{'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4681 (+/-0.0091)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5092 (+/-0.0106)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.5012 (+/-0.0172)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.5031 (+/-0.0178)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.5166 (+/-0.0092)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.5051 (+/-0.0125)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.5022 (+/-0.0170)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5021 (+/-0.0198)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4949 (+/-0.0156)\n",
      "{'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4976 (+/-0.0205)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.5108 (+/-0.0153)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.5011 (+/-0.0146)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4949 (+/-0.0062)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.5001 (+/-0.0106)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.5031 (+/-0.0140)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4903 (+/-0.0067)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.5012 (+/-0.0163)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4902 (+/-0.0064)\n",
      "{'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4784 (+/-0.0055)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50} -> F1-macro: 0.4859 (+/-0.0132)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100} -> F1-macro: 0.4814 (+/-0.0090)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200} -> F1-macro: 0.4798 (+/-0.0108)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50} -> F1-macro: 0.4859 (+/-0.0132)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 100} -> F1-macro: 0.4814 (+/-0.0090)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 200} -> F1-macro: 0.4798 (+/-0.0108)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50} -> F1-macro: 0.4928 (+/-0.0093)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100} -> F1-macro: 0.4703 (+/-0.0159)\n",
      "{'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} -> F1-macro: 0.4668 (+/-0.0077)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20f54c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "F1-macro scores por fold: [0.52904986 0.50113379 0.50697796 0.47613209 0.48690211]\n",
      "F1-macro promedio: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Random Forest\n",
    "rf_model = RandomForestClassifier(max_depth=20, min_samples_leaf=1, min_samples_split=2,n_estimators=100, random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "rf_scores = cross_val_score(rf_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Random Forest:\")\n",
    "print(f\"F1-macro scores por fold: {rf_scores}\")\n",
    "print(f\"F1-macro promedio: {rf_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636070f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7732    0.9820    0.8652       500\n",
      "           1     0.8615    0.2800    0.4226       200\n",
      "\n",
      "    accuracy                         0.7814       700\n",
      "   macro avg     0.8174    0.6310    0.6439       700\n",
      "weighted avg     0.7985    0.7814    0.7388       700\n",
      "\n",
      "Matriz de confusión:\n",
      "[[491   9]\n",
      " [144  56]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de entrenamiento\n",
    "rf_model.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Predecir sobre el conjunto de validación/desarrollo\n",
    "y_pred = rf_model.predict(X_dev_reduced)\n",
    "y_true = dev_df['Tag Value']\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299696c7",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ad7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Mejor configuración encontrada:\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Mejor F1-macro: 0.6930\n",
      "\n",
      "Resultados detallados por configuración:\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.4522 (+/-0.0272)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.4538 (+/-0.0278)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5510 (+/-0.0155)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5456 (+/-0.0184)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.5946 (+/-0.0134)\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.5922 (+/-0.0128)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.5079 (+/-0.0218)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.4939 (+/-0.0239)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5794 (+/-0.0146)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5652 (+/-0.0159)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6180 (+/-0.0150)\n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6137 (+/-0.0124)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.5182 (+/-0.0216)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.5208 (+/-0.0156)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.5943 (+/-0.0181)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.5996 (+/-0.0213)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6565 (+/-0.0277)\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6490 (+/-0.0276)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6171 (+/-0.0235)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6056 (+/-0.0169)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6482 (+/-0.0193)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6361 (+/-0.0176)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6748 (+/-0.0183)\n",
      "{'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6568 (+/-0.0136)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6366 (+/-0.0188)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6203 (+/-0.0096)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6676 (+/-0.0255)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6531 (+/-0.0201)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6826 (+/-0.0172)\n",
      "{'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6710 (+/-0.0173)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6609 (+/-0.0241)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6625 (+/-0.0245)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6672 (+/-0.0247)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6709 (+/-0.0226)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6781 (+/-0.0241)\n",
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6817 (+/-0.0219)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6525 (+/-0.0229)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6349 (+/-0.0155)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6756 (+/-0.0173)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6630 (+/-0.0132)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6930 (+/-0.0227)\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6833 (+/-0.0138)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6756 (+/-0.0221)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6576 (+/-0.0167)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6841 (+/-0.0111)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6760 (+/-0.0170)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6876 (+/-0.0186)\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6923 (+/-0.0158)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.8} -> F1-macro: 0.6764 (+/-0.0263)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1.0} -> F1-macro: 0.6628 (+/-0.0244)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8} -> F1-macro: 0.6733 (+/-0.0281)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0} -> F1-macro: 0.6769 (+/-0.0223)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8} -> F1-macro: 0.6805 (+/-0.0325)\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0} -> F1-macro: 0.6805 (+/-0.0240)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# Definir el modelo y los hiperparámetros a probar\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [2, 3, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Configurar validación cruzada y búsqueda de hiperparámetros\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_search = GridSearchCV(\n",
    "    gb,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='macro'),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "grid_search.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Imprimir mejores resultados\n",
    "print(\"Mejor configuración encontrada:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Mejor F1-macro: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nResultados detallados por configuración:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(f\"{param} -> F1-macro: {mean:.4f} (+/-{std:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c416888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting:\n",
      "F1-macro scores por fold: [0.69137246 0.67957478 0.68181818 0.66277605 0.69608504]\n",
      "F1-macro promedio: 0.6823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir el modelo de Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(subsample=0.8,n_estimators=200, learning_rate=0.1, max_depth=5, random_state=0)\n",
    "\n",
    "# Evaluar el modelo con validación cruzada usando las representaciones reducidas (X_train_reduced)\n",
    "gb_scores = cross_val_score(gb_model, X_train_reduced, train_df['Tag Value'], cv=5, scoring='f1_macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Gradient Boosting:\")\n",
    "print(f\"F1-macro scores por fold: {gb_scores}\")\n",
    "print(f\"F1-macro promedio: {gb_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45ebc007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7279    0.9040    0.8064       500\n",
      "           1     0.3924    0.1550    0.2222       200\n",
      "\n",
      "    accuracy                         0.6900       700\n",
      "   macro avg     0.5601    0.5295    0.5143       700\n",
      "weighted avg     0.6320    0.6900    0.6395       700\n",
      "\n",
      "Matriz de confusión:\n",
      "[[452  48]\n",
      " [169  31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Entrenar el modelo en todo el conjunto de entrenamiento\n",
    "gb_model.fit(X_train_reduced, train_df['Tag Value'])\n",
    "\n",
    "# Predecir sobre el conjunto de validación/desarrollo\n",
    "y_pred = gb_model.predict(X_dev_reduced)\n",
    "y_true = dev_df['Tag Value']\n",
    "\n",
    "# Imprimir el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
